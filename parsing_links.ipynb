{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bfee5f",
   "metadata": {},
   "source": [
    "<h1>Parsing<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4eed95",
   "metadata": {},
   "source": [
    "Суммарно парсила сайт 2 дня. Финальный парсинг занял 4 часа.\n",
    "Но давайте начнем с самого начала.\n",
    "Первым делом я пошла на сайт https://www.kaggle.com/datasets/joebeachcapital/ign-games/data?select=ign.csv и\n",
    "просто забрала оттуда датасет\n",
    "Далее мне нужно было его обогатить, поскольку там данные только про игры до 2016 года.\n",
    "Я отправилась на сайт https://www.ign.com/reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c53858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c783fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# С сайта https://selenium-python.readthedocs.io/getting-started.html#simple-usage\n",
    "# на всякий случай предварительно качаю всё\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934bd46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Давайте начнем с самого начала.\n",
    "Первым делом я пошла на сайт https://www.kaggle.com/datasets/joebeachcapital/ign-games/data?select=ign.csv и\n",
    "просто забрала оттуда датасет\n",
    "Далее мне нужно было его обогатить, поскольку там данные только про игры до 2016 года.\n",
    "Я отправилась на сайт https://www.ign.com/reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4866a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_u_gen():\n",
    "    with open(\"../All_data_files/ign.csv\", \"r\", encoding=\"utf-8\") as ign:\n",
    "        df = pd.read_csv(ign)\n",
    "        unique_genres = list(df[\"genre\"].unique())\n",
    "        uni_gen = [x for x in unique_genres if isinstance(x, str) and \", \" not in x]\n",
    "    return uni_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ccc44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_u_pltf():\n",
    "    with open(\"../All_data_files/ign.csv\", \"r\", encoding=\"utf-8\") as ign:\n",
    "        df = pd.read_csv(ign)\n",
    "        unique_pltf = list(df[\"platform\"].unique())\n",
    "        uni_pl = [x for x in unique_pltf if isinstance(x, str)]\n",
    "    return uni_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9591d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = find_u_gen()\n",
    "unique_platforms = find_u_pltf()\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "    \"Accept-Encoding\": \"*\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "url = \"https://www.ign.com/reviews\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419c426",
   "metadata": {},
   "source": [
    "**Далее идет кусок кода, который я делала на Pycharm. Весь парсинг занял много часов, поэтому когда я переносила файлы в Jupiter, я решила, что этот кусок нет смысла прогонять заново**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a950d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для того, чтобы начать парсинг, я буду использовать модуль fake_useragent.\n",
    "response = requests.get(url, headers={'User-Agent': UserAgent().chrome})\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce61f8",
   "metadata": {},
   "source": [
    " Я решила парсить сайт со всеми обзорами, а не только игровыми,\n",
    " так как страница с игровыми обзорами не парсилась дальше 17-ой страницы.\n",
    "Я не смогла найти решение, поэтому пошла через главную страницу.\n",
    "Через fake_useragent получается обмануть сайт, тут не пришлось выдумывать каких-то сложных масок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6062845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# однако тут всплывает новая проблема. Сайт который я хочу парсить - динамический. То есть он прогружается при скролинге\n",
    "# Это очень печально, потому что тут даже нельзя пощелкать страницы, чтобы найти их значения. Но я не отчаиваюсь\n",
    "# и иду в интернет искать, что делать. Мне повезло - на сайте IGN нет явной пагинации, спасибо...\n",
    "# Принято решение использовать селениум\n",
    "# Устанавливаю\n",
    "# Нужно настроить связь питона и браузера, поэтому:\n",
    "#************************************\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={UserAgent().random}\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d047eee",
   "metadata": {},
   "source": [
    " Далее я столкнулась с проблемой - сайт не имеет явных страниц, его надо прокручивать.\n",
    " Немного поборовшись с ним, получилось сделать то, что мне нужно.\n",
    "Но возникла новая проблема - сайт парсится долго, а сколько конкретно мне нужно прокруток я не знаю.\n",
    "Поставила 600 наугад, потому что вручную прокрутить и проверить было бы еще сложнее\n",
    "В итоге я смогла собрать данные по всем недостающим мне годам\n",
    "(получилось, что дата обзора не совпадала с датой выхода, что, наверное, должно было быть очевидно,\n",
    "так что у меня появилось 5352 новых ссылок, которые покрыли вообще все годы с 1970 до 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175b3b3",
   "metadata": {},
   "source": [
    "так как у меня динамический сайт, делаю вот так: https://digitalsfera.ru/prokrutka-stranitsy-v-selenium-python\n",
    "я взяла код отсюда, добавила ограничение на количество страниц,\n",
    "также я добавила в конец скрипт для записи всех ссылок в отдельный файл ign_links_more.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23606809",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "pages = [i for i in range(20)]\n",
    "n = 0\n",
    "while n < 600:\n",
    "    # Прокрутка вниз\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Пауза, пока загрузится страница.\n",
    "    # Вычисляем новую высоту прокрутки и сравниваем с последней высотой прокрутки.\n",
    "    time.sleep(3)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    n += 1\n",
    "    print(n)\n",
    "    print(last_height, new_height)\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ign_links_more.txt\", \"w\", encoding=\"utf-8\") as ign:\n",
    "    # найдем все ссылки на каждую страницу и сложим их в новый файл\n",
    "    start_info = driver.find_elements(By.CLASS_NAME, \"item-body\")\n",
    "    for string in start_info:\n",
    "        href = string.get_attribute(\"href\")\n",
    "        ign.write(href + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
